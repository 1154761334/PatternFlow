{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_for_dataset(path_train,path_seg):\n",
    "    \"\"\"This Function is used Extracting the file names of the images and masks in training, test and validation folders\"\"\"\n",
    "    \n",
    "    isic_train = next(os.walk(path_train))[2] # returns all the files \"DIR.\"\n",
    "    isic_seg_train=next(os.walk(path_seg))[2] # returns all the files \"DIR.\"\n",
    "\n",
    "    print(\"No. of images in training folder= \",len(isic_train))\n",
    "    print(\"No. of images in test folder= \",len(isic_seg_train))\n",
    "    return isic_train, isic_seg_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_test(isic_train, isic_seg_train):\n",
    "    \"\"\"This Function is used for Sorting the data with respect to labels\"\"\"\n",
    "    isic_train_sort=sorted(isic_train) # Sorting of data with respect to labels\n",
    "    isic_seg_train_sort=sorted(isic_seg_train) # Sorting of data with respect to labels\n",
    "    \n",
    "    return isic_train_sort, isic_seg_train_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_img(inp_path,isic):\n",
    "    \"\"\" This function is used for Loading the images from the Training_Input_x2 folder\"\"\"\n",
    "    \" - Storing them with the above dimensions specified\"\n",
    "    \" - Loading the images in Greayscale format\"\n",
    "    \" - Normalizing the image with 255 as Normalising data by dividing it by 255 should improve activation functions performance\"\n",
    "    \" - Sigmoid function works more efficiently with data range 0.0-1.0.\"\n",
    "    \n",
    "    X_ISIC_train= np.zeros((len(isic),img_height,img_width,1),dtype=np.float32)\n",
    "    for n, id_ in tqdm_notebook(enumerate(isic), total=len(isic)): # capture all the images ids using tqdm\n",
    "        img = load_img(inp_path+id_, color_mode = 'grayscale')  # Load images here\n",
    "        x_img = img_to_array(img) # Convert images to array\n",
    "        x_img = resize(x_img,(256,256,1),mode = 'constant',preserve_range = True)\n",
    "        X_ISIC_train[n] = x_img/255 # Normalize the images\n",
    "        \n",
    "    return X_ISIC_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_segmentation(inp_path,isic):\n",
    "    \"\"\" This function is used for Loading the images from the Training_GroundTruth_x2 folder\"\"\"\n",
    "    \" - Storing them with the above dimensions specified\"\n",
    "    \" - Loading the images in Greayscale format\"\n",
    "    \n",
    "    Y_ISIC_train= np.zeros((len(isic),img_height,img_width,1),dtype=np.uint8)\n",
    "    for n, id_ in tqdm_notebook(enumerate(isic), total=len(isic)):\n",
    "        # Load images\n",
    "        img = load_img(inp_path+id_,color_mode = 'grayscale')\n",
    "        x_img = img_to_array(img)\n",
    "        x_img = resize(x_img,(256,256,1),mode = 'constant', preserve_range = True)\n",
    "        Y_ISIC_train[n] = x_img\n",
    "        \n",
    "    return Y_ISIC_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(X_ISIC_train, Y_ISIC_train):\n",
    "    \"\"\"This Function is used for spliting the dataset into Train, Test, and Val\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ISIC_train, Y_ISIC_train, test_size=0.20, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    X_train.shape\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(y_train,y_test,y_val):\n",
    "    \"\"\"This Function is used for Pre_processing the Y labels into two categoraical format\"\"\"\n",
    "    \" - By calculating the quotient \"\n",
    "    \" - By using One-Hot Encoding to Labels\"\n",
    "    Y_ISIC_train_sc = y_train//255\n",
    "    Y_ISIC_test_sc = y_test//255\n",
    "    Y_ISIC_val_sc = y_val//255\n",
    "    Y_ISIC_train_cat = to_categorical(Y_ISIC_train_sc) # one hot encoding\n",
    "    Y_ISIC_test_cat = to_categorical(Y_ISIC_test_sc) # one hot encoding\n",
    "    Y_ISIC_val_cat = to_categorical(Y_ISIC_val_sc) # one hot encoding\n",
    "    \n",
    "    return Y_ISIC_train_cat, Y_ISIC_test_cat, Y_ISIC_val_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dice Coeffient\n",
    "from keras import backend as K\n",
    "def dice_coeff(y_true, y_pred, smooth=1):\n",
    "    \"\"\"This Function is used to gauge similarity of two samples\"\"\"\n",
    "    \"When applied to Boolean data, using the definition of true positive (TP), false positive (FP), and false negative (FN)\"\n",
    "    \n",
    "    intersect = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersect\n",
    "    coeff_dice = K.mean((intersect + smooth) / (union + smooth), axis=0)\n",
    "    return coeff_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, smooth = 1):\n",
    "    \"\"\"This Function returns the dice loss by subtracting the dice_coeff similarity by 1 of the two samples\"\"\"\n",
    "    \n",
    "    return 1 - dice_coeff(y_true, y_pred, smooth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    \"\"\"This Function is used to define the Conv2D layers by defining the filters, kernalsize and kernal initilizer\"\"\"\n",
    "    \" - It defines the BatchNorm of the previous layer\"\n",
    "    \" - It defines the Activation function(relu) to be used in the Conv2D layer\"\n",
    "    \n",
    "    # first layer\n",
    "    layer = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    # It draws samples from a truncated normal distribution centered on 0 \n",
    "    if batchnorm:\n",
    "        layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    # second layer\n",
    "    layer = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generat_unet(input_img, n_filters=16, dropout=0.1, batchnorm=True):\n",
    "    \"\"\"This Function is used to define the Architecture of the U-Net model for the Contracting and Expanding path\"\"\"\n",
    "    \" - It defines the Kernal size to be 3*3\"\n",
    "    \" - It defines the MaxPooling of (2,2) strides\"\n",
    "    \" - It defines the DropOut layer of 0.05\"\n",
    "    # contracting path - reduce enoder part\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    # expansive path - Decoder part\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(2, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(results):\n",
    "    \"\"\"This Function is used for used for plotting the graph of the training and Validation loss with respect to epoch\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Dice_Loss\")\n",
    "    plt.plot(results.history[\"loss\"], label=\"training_loss\")\n",
    "    plt.plot(results.history[\"val_loss\"], label=\"validation_loss\")\n",
    "    plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_plot(results):\n",
    "    \"\"\"This Function is used for Plotting the training and validation accuracy with respect to epochs\"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Classification Accuracy\")\n",
    "    plt.plot(results.history[\"accuracy\"],label=\"training_accuracy\")\n",
    "    plt.plot(results.history[\"val_accuracy\"],label=\"validation_accuracy\")\n",
    "    plt.plot(np.argmin(results.history[\"val_accuracy\"]),np.max(results.history[\"val_accuracy\"]),marker=\"x\",color=\"r\",label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(model,X_test,y_test):\n",
    "    \"\"\"This Function is used for Capturing the Best model for the epoch\"\"\"\n",
    "    model.load_weights('model-ISIC.h5')\n",
    "    test_preds=model.predict(X_test,verbose=1) # predict the model\n",
    "    test_preds_max=np.argmax(test_preds,axis=-1) # Returns the indices of the maximum values along an axis\n",
    "    n,h,w,g=y_test.shape\n",
    "    test_preds_reshape=test_preds_max.reshape(n,h,w,g)\n",
    "    return test_preds_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ISIc(X, y, Y_pred,ix=None):\n",
    "    \"\"\"This function is used for ploting the True image vs the Predictive image from the above model\"\"\"\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "    else:\n",
    "        ix = ix\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='gray')\n",
    "    ax[0].contour(X[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[0].set_title('Input Image')\n",
    "    \n",
    "    \n",
    "    ax[1].imshow(y[ix, ..., 0], cmap='gray')\n",
    "    ax[1].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[1].set_title('True Image')\n",
    "    \n",
    "    ax[2].imshow(Y_pred[ix, ..., 0], cmap='gray')\n",
    "    ax[2].contour(Y_pred[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[2].set_title('Predicted Image')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
