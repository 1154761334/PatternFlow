{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JD_hk2Yohtnl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP3710 Project\n",
    "# Question 4: Segment the ISICs data set with the Improved UNet [1] with all labels having a minimum Dice similarity\n",
    "# coefficient of 0.8 on the test set. [Normal Difficulty]\n",
    "\n",
    "# Student Name: Xiao Sun\n",
    "# Studeng Number: 45642586\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2596, 2596)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "img_GroundTruth = os.listdir(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1_Training_GroundTruth_x2')\n",
    "img_input = os.listdir(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1-2_Training_Input_x2')\n",
    "\n",
    "len(img_GroundTruth), len(img_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ATTRIBUTION.txt', 'LICENSE.txt')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_GroundTruth[0], img_GroundTruth[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    \n",
    "    # we use img_input[1:-1] because the first and last file is not image document.\n",
    "    #load input images and process into tf dataset.\n",
    "    img_input = os.listdir(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1-2_Training_Input_x2')\n",
    "    img_input = [os.path.join(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1-2_Training_Input_x2', path) for path in img_input[1:-1]]\n",
    "    path_img_input = tf.data.Dataset.from_tensor_slices(img_input)\n",
    "    image_input_ds = path_img_input.map(data_processing_norm_input, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    #load mask images and process into tf dataset.\n",
    "    img_GroundTruth = os.listdir(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1_Training_GroundTruth_x2')\n",
    "    img_GroundTruth = [os.path.join(r'C:\\Users\\s4564258\\.keras\\datasets\\ISIC2018_Task1_Training_GroundTruth_x2', path) for path in img_GroundTruth[1:-1]]\n",
    "    path_img_GroundTruth = tf.data.Dataset.from_tensor_slices(img_GroundTruth)\n",
    "    image_mask_ds = path_img_GroundTruth.map(data_processing_norm_GT, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    # implot_show(image_input_ds.take(4))\n",
    "    # implot_show(image_mask_ds.take(4))\n",
    "    \n",
    "    return image_input_ds, image_mask_ds\n",
    "    \n",
    "def data_processing_norm_input(image):\n",
    "    # process input img data into tf tensor, and normalization.\n",
    "    \n",
    "    img_raw = tf.io.read_file(image)\n",
    "    image = tf.image.decode_jpeg(img_raw, channels=3)\n",
    "    image = tf.image.resize(image, [192, 192])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    \n",
    "    return image\n",
    "    \n",
    "    \n",
    "def data_processing_norm_GT(image):\n",
    "    # process mask (GroundTruth) img data into tf tensor, and normalization.\n",
    "    \n",
    "    img_raw = tf.io.read_file(image)\n",
    "    image = tf.image.decode_jpeg(img_raw, channels=1)\n",
    "    image = tf.image.resize(image, [192, 192])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    \n",
    "    return image\n",
    "    \n",
    "def implot_show(ds):\n",
    "    # using imshow to vertify correctly load and process data\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    for n, image in enumerate(ds):\n",
    "        print(image.shape)\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.imshow(image)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(image_ds):\n",
    "    # split the whole tf data set into train, validation and test.\n",
    "    \n",
    "    # this step will slow down the process.\n",
    "    # size = len(list(image_ds))\n",
    "    size = 2594\n",
    "    \n",
    "    train_size = int(0.7 * size)\n",
    "    val_size = int(0.15 * size)\n",
    "    test_size = int(0.15 * size)\n",
    "    \n",
    "    train_image = image_ds.take(train_size)\n",
    "    val_image = image_ds.skip(train_size)\n",
    "    test_image = val_image.take(test_size)\n",
    "    val_image = val_image.skip(test_size)\n",
    "    \n",
    "    return train_image, val_image, test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input_ds, image_mask_ds = load_data()\n",
    "\n",
    "input_train, input_val, input_test = split_train_test_val(image_input_ds)\n",
    "mask_train, mask_val, mask_test = split_train_test_val(image_mask_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet_context_module(filters, inp, layer_name):\n",
    "    # Each context_module consists of two 3x3 conv layers and a dropout(0.3) in between.\n",
    "    \n",
    "    x1 = layers.Conv2D(filters, kernel_size =3, padding = 'same')(inp)\n",
    "    # x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x1 = layers.layers.Dropout(.3)(x1)\n",
    "    x2 = layers.Conv2D(filters, kernel_size =3, padding = 'same')(x1)\n",
    "    # x2 = layers.BatchNormalization()(x2)\n",
    "    x2 = layers.Activation('relu')(x2)\n",
    "    x2 = layers.layers.Dropout(.3)(x2)\n",
    "\n",
    "    return x2\n",
    "    \n",
    "def UNet_upsampling_module():\n",
    "    # ...It is like a layer that combines the UpSampling2D and Conv2D layers into one layer. \n",
    "    \n",
    "    # what twice means in paper?\n",
    "    x1 = layers.UpSampling2D(size=(2,2))(inp)\n",
    "    x2 = layers.Conv2D(filters, kernel_size =3, padding = 'same')(x1)\n",
    "    \n",
    "    return x2\n",
    "    \n",
    "    \n",
    "def UNet_localization_module():\n",
    "    # A localization module consists of a 3x3x3 convolution followed by a 1x1x1 convolution that halves the\n",
    "    # number of feature maps.\n",
    "    \n",
    "    x1 = layers.Conv2D(filters, kernel_size =3, padding = 'same')(inp)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x2 = layers.Conv2D(filters, kernel_size =1, padding = 'same')(x1)\n",
    "    x2 = layers.BatchNormalization()(x2)\n",
    "    x2 = layers.Activation('relu')(x2)\n",
    "    \n",
    "    return x2\n",
    "    \n",
    "    \n",
    "\n",
    "def UNet_segmentation_module():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Improved_UNet_model():\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2594"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input_ds, image_mask_ds\n",
    "len(list(image_input_ds))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP3710_demo1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
